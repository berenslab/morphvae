{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2SeqDataSet, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from itertools import product\n",
    "from utils.training_utils import create_Seq2SeqDataset, train, evaluate\n",
    "from datetime import datetime\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import training_utils\n",
    "reload(training_utils)\n",
    "from training_utils import train, evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "INPUT_DIM = 3   \n",
    "lr = 1e-2                           # learning rate\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 3\n",
    "N_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/toy_data/3_populations/iterator/val_iterator.pkl', 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "\n",
    "with open('./data/toy_data/3_populations/iterator/train_iterator.pkl', 'rb') as f:\n",
    "    train_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "MASKING_ELEMENT = train_iterator.dataset.masking_el\n",
    "n_walks = train_iterator.dataset.n_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with parameters:     EMD_D = HID_D = 16, LAT_D = 16, Dropout= 0.1, kappa= 500, pooling=max\n",
      "KLD: 29.22101402282715\n"
     ]
    }
   ],
   "source": [
    "# for each parameter combination, train 3 models\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "while len(parameter_grid) > 0:\n",
    "    \n",
    "    # get next parameter set\n",
    "    emb_dim = int(parameter_grid[0][0])\n",
    "    latent_dim = int(parameter_grid[0][1])\n",
    "    dpout = float(parameter_grid[0][2])\n",
    "    kappa = int(parameter_grid[0][3])\n",
    "    pool = parameter_grid[0][4]\n",
    "    \n",
    "    print('Fitting model with parameters: \\\n",
    "    EMD_D = HID_D = %i, LAT_D = %i, Dropout= %.1f, kappa= %i, pooling=%s'%(emb_dim, \n",
    "                                                                           latent_dim, \n",
    "                                                                           dpout, \n",
    "                                                                           kappa, \n",
    "                                                                           pool))\n",
    "    # save the file without this parameter set\n",
    "    with open('./models/parameter_search/parameter_grid.txt', 'wb') as f: \n",
    "        np.save(f,parameter_grid[1:])\n",
    "        \n",
    "    for k in range(3):\n",
    "        start = datetime.now()\n",
    "        # model\n",
    "        enc = SeqEncoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dec = SeqDecoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dist = vMF(latent_dim, kappa=kappa)\n",
    "        model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "        classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)\n",
    "        \n",
    "        # initialize model \n",
    "        model.apply(init_weights)\n",
    "        classifier.apply(init_weights)\n",
    "        \n",
    "        # losses\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        mse_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "\n",
    "        ### train the model(s)\n",
    "\n",
    "        best_test_loss = np.infty\n",
    "        training = []\n",
    "        validation = []\n",
    "        for e in range(N_EPOCHS):\n",
    "\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, 1,1.)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss)\n",
    "\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "\n",
    "            training += [[train_loss, train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            #print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {val_loss:.2f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "            \n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                suffix = 'emb%i_hid%i_lat%i_dp%.1f_k%i_%s'%(emb_dim,emb_dim,latent_dim,dpout,kappa,pool)\n",
    "                torch.save({'epoch': e,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'classifier_state_dict': classifier.state_dict()\n",
    "                           }, './models/parameter_search/%s_run%i_best.pt'%(suffix,(k+1)))\n",
    "        # save training and validation loss\n",
    "        validation_ = np.array(validation)\n",
    "        training_ = np.array(training)\n",
    "        losses = np.concatenate((training_, validation_), axis=1)\n",
    "        # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "        # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "        with open('./models/parameter_search/losses_%s_%i.npy'%(suffix, (k+1)), 'wb') as f:\n",
    "            np.save(f,losses)\n",
    "            \n",
    "        end = datetime.now()\n",
    "        print('Time to fit model %i : '%(k+1), end-start)\n",
    "    torch.cuda.empty_cache()\n",
    "    with open('./models/parameter_search/parameter_grid.txt', 'rb') as f: \n",
    "        parameter_grid = np.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
