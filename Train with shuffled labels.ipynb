{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2SeqDataSet, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from itertools import product\n",
    "from utils.training_utils import train, evaluate\n",
    "from datetime import datetime\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import training_utils\n",
    "# reload(training_utils)\n",
    "# from training_utils import train, evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "folder = '3_populations'\n",
    "with open('./data/toy_data/%s/iterator/shuffled_val_iterator.pkl'%(folder), 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "\n",
    "with open('./data/toy_data/%s/iterator/shuffled_train_iterator.pkl'%(folder), 'rb') as f:\n",
    "    train_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "n_walks = train_iterator.dataset.n_walks\n",
    "# parameter\n",
    "INPUT_DIM = 3   \n",
    "lr = 1e-2                           # learning rate\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 3\n",
    "N_EPOCHS = 150\n",
    "MASKING_ELEMENT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 45.709938049316406\n",
      "Epoch 0, Train Loss: 1360.344875000, Test Loss: 8337.006000000\n",
      "Epoch 1, Train Loss: 1003.453680556, Test Loss: 6842.868500000\n",
      "Epoch 2, Train Loss: 839.720486111, Test Loss: 7049.564500000\n",
      "Epoch 3, Train Loss: 720.715430556, Test Loss: 6775.543000000\n",
      "Epoch 4, Train Loss: 676.795076389, Test Loss: 7422.435000000\n",
      "Epoch 5, Train Loss: 633.520416667, Test Loss: 7328.762500000\n",
      "Epoch 6, Train Loss: 628.223395833, Test Loss: 8124.971000000\n",
      "Epoch 7, Train Loss: 604.718465278, Test Loss: 6883.512000000\n",
      "Epoch 8, Train Loss: 605.018402778, Test Loss: 7300.894500000\n",
      "Epoch 9, Train Loss: 593.012541667, Test Loss: 7053.825000000\n",
      "Epoch 10, Train Loss: 585.497750000, Test Loss: 7017.565000000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "emb_dim = 32\n",
    "latent_dim = 32\n",
    "dpout = .1\n",
    "kappa = 500\n",
    "pool = 'max'\n",
    "    \n",
    "### train the model(s)\n",
    "for frac in [1.]:\n",
    "    for k in range(1,4):\n",
    "        start = datetime.now()\n",
    "        # model\n",
    "        enc = SeqEncoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dec = SeqDecoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dist = vMF(latent_dim, kappa=kappa)\n",
    "        model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "        classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)\n",
    "\n",
    "        # initialize model \n",
    "        model.apply(init_weights)\n",
    "        classifier.apply(init_weights)\n",
    "\n",
    "        # losses\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        mse_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "\n",
    "        best_test_loss = np.infty\n",
    "        N_EPOCHS= 150\n",
    "        training = []\n",
    "        validation = []\n",
    "        for e in range(N_EPOCHS):\n",
    "\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, \n",
    "                                                 clip=1,norm_p=None, class_fraction=frac)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss, norm_p=None)\n",
    "\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "\n",
    "            training += [[train_loss, train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            print(f'Epoch {e}, Train Loss: {train_loss:.9f}, Test Loss: {val_loss:.9f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0 :\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "\n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                suffix = 'shuffled_emb%i_hid%i_lat%i_dp%.1f_k%i_%s'%(emb_dim,emb_dim,latent_dim,dpout,kappa,pool)\n",
    "                suffix += '_frac%.1f'%frac\n",
    "                suffix += '_unscaled'\n",
    "                suffix += '_sum'\n",
    "                \n",
    "                torch.save({'epoch': e,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'classifier_state_dict': classifier.state_dict()\n",
    "                               }, './models/%s/%s_run%i_best.pt'%(folder,suffix,(k+1)))\n",
    "                # save training and validation loss\n",
    "                validation_ = np.array(validation)\n",
    "                training_ = np.array(training)\n",
    "                losses = np.concatenate((training_, validation_), axis=1)\n",
    "                # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "                # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "                with open('./models/%s/shuffled_losses_%s_%i.npy'%(folder, suffix, (k+1)), 'wb') as f:\n",
    "                    np.save(f,losses)\n",
    "\n",
    "        validation_ = np.array(validation)\n",
    "        training_ = np.array(training)\n",
    "        losses = np.concatenate((training_, validation_), axis=1)\n",
    "        # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "        # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "        with open('./models/%s/shuffled_losses_%s_%i.npy'%(folder,suffix, (k+1)), 'wb') as f:\n",
    "                np.save(f,losses)\n",
    "        end = datetime.now()\n",
    "        print('Time to fit model %i : '%(k+1), end-start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
